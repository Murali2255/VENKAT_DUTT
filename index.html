<!DOCTYPE html>
<html lang="en">

<head>

  <link rel="icon" type="image/png" href="/favicon.png" />
  <meta charset="utf-8" />
  <meta content="width=device-width, initial-scale=1.0" name="viewport" />

  <title>Venkat Dutt</title>
  <meta content="" name="descriptison" />
  <meta content="" name="keywords" />

  <!-- Google Fonts -->
  <link
    href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Raleway:300,300i,400,400i,500,500i,600,600i,700,700i|Poppins:300,300i,400,400i,500,500i,600,600i,700,700i"
    rel="stylesheet" />

  <!-- Vendor CSS Files -->
  <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet" />
  <link href="assets/vendor/icofont/icofont.min.css" rel="stylesheet" />
  <link href="assets/vendor/remixicon/remixicon.css" rel="stylesheet" />
  <link href="assets/vendor/owl.carousel/assets/owl.carousel.min.css" rel="stylesheet" />
  <link href="assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet" />
  <link href="assets/vendor/venobox/venobox.css" rel="stylesheet" />


  <link rel="stylesheet" href="assets/css/styles3.css" />

</head>

<body>

  <header id="header" class="header-tops">
    <div class="container">
      <h4 id="intro-greeting" style="
            font-size: 36px;
            margin-top: 0px;
            padding: 0;
            line-height: 1;
            font-weight: 700;
            font-family: 'Poppins', sans-serif;
          ">
        Hi, I'm
      </h4>
      <h1><a href="index.html" id="full-name">Venkat Dutt</a></h1>

      <nav class="nav-menu d-none d-lg-block">
        <ul>
          <li class="active">
            <a href="#header"> <span>Home</span></a>
          </li>
          <li>
            <a href="#about"><span>About</span></a>
          </li>

          <li>
            <a href="#experience"> <span>Experience</span></a>
          </li>

          <li>
            <a href="#skills"> <span>Skills</span></a>
          </li>
          <li>
            <a href="./assets/resume/Venkat_Dutt.pdf" target="_blank">
              <span>Resume</span></a>
          </li>

        </ul>
      </nav>
      <!-- .nav-menu -->

      <div class="social-links">
        <a href="#" target="_blank" class="linkedin"><i class="bx bxl-linkedin"></i></a>

        <a href="mailto:duttvenkat2k23@gmail.com" target="_blank" class="google"><i class="bx bxl-google"></i></a>
      </div>
    </div>
  </header>
  <!-- End Header -->

  <!-- ======= About Section ======= -->
  <section id="about" class="about">
    <!-- ======= About Me ======= -->
    <div class="about-me container">
      <div class="section-title">
        <br>
        <h2>About</h2>
      </div>

      <div class="row">
        <div class="col-lg-4" data-aos="fade-right">
          <img src="assets/img/Venkat_profile.jpeg" class="img-fluid" alt="Uday Photo" />
        </div>

        <div class="col-lg-8 pt-4 pt-lg-0 content" data-aos="fade-left" style="font-weight: 500; color: #020a2d">
          <ul style="list-style: none;">


            <li>
              <i class="icofont-rounded-right"></i>
              8+ years of experience as Big Data Engineer /Data Engineer including designing, developing and
              implementation of data models for enterprise-level applications and systems.
            </li>
            <li>
              <i class="icofont-rounded-right"></i>
              Experience in using cloud services Amazon Web Services (AWS) including EC2, 53, AWS Lambda
              and EMR, used Redshift for migration.

            </li>
            <li>
              <i class="icofont-rounded-right"></i>
              Experience in creating complex data pipeline process using T-SQL scripts, SSIS packages, Aptery
              workflow, PL/SQL scripts, Cloud REST APIs, Python scripts, GCP Composer, GCP dataflow.
            </li>
            <li>
              <i class="icofont-rounded-right"></i>
              Experience in building ETL systems using python and in-memory computing framework (Apache
              Spark), scheduling and maintaining data pipelines at regular intervals in Apache Airflow.
            </li>
            <li>
              <i class="icofont-rounded-right"></i>
              Experience in analyzing data using Spark SQL, HIVEQL, PIG Latin, Spark/Scala and custom Map
              Reduce programs in Java.
            </li>
            <li>
              <i class="icofont-rounded-right"></i>
              Experience in Creating pipelines, data flows and complex data transformations and manipulations
              using ADF and PySpark with Databricks.
            </li>
            <li>
              <i class="icofont-rounded-right"></i>
              Experience of using CI/CD techniques and processes DevOps/Git repository code promotion.
            </li>
            <li>
              <i class="icofont-rounded-right"></i>
              Experienced with version control systems like Git, GitHub, to keep the versions and configurations
              of the code organized
            </li>
            <li>
              <i class="icofont-rounded-right"></i>
              Experience with structured (MySQL, Oracle SQL, PostgreSQL) and unstructured (NoSQL)
              databases. Strong understanding of relational databases.
            </li>
          </ul>
          <div class="row">
            <div class="col-lg-6">
              <ul>

                <li>
                  <i class="icofont-rounded-right"></i>
                  <strong>City:</strong> Fort Wayne, Indiana 46835, USA
                </li>
              </ul>
            </div>
            <div class="col-lg-6">
              <ul>
                <li>
                  <i class="icofont-rounded-right"></i>
                  <strong>Email:</strong> duttvenkat2k23@gmail.com
                </li>
              </ul>
            </div>
          </div>
          <div class="row">
            <div class="col-lg-6">
              <ul>

                <li>
                  <i class="icofont-rounded-right"></i>
                  <strong>LinkedIn:</strong> LinkedIn
                </li>
              </ul>
            </div>
            <div class="col-lg-6">
              <ul>
                <li>
                  <i class="icofont-rounded-right"></i>
                  <strong>Call:</strong> (573) 723-1868
                </li>
              </ul>
            </div>
          </div>
        </div>
      </div>
    </div>
    <!-- End About Me -->

    <!-- ======= Interests ======= -->
    <div class="interests container">
      <div class="section-title">
        <br>
        <h2>Education</h2>
      </div>

      <div class="row">
        <div class="col-lg-6 col-md-6">

          <ul style="list-style:  none;">
            <li><i class="icofont-rounded-right"></i> Master’s in Data Analytics, University of Texas, Dallas, United
              States. </li>
            <li> <i class="icofont-rounded-right"></i>January 2017 – Apr 2018</li>
          </ul>


        </div>
        <div class="col-lg-6 col-md-6">

          <ul style="list-style:  none;">
            <li><i class="icofont-rounded-right"></i> Bachelors in Computer Science Engineering, VIT, Vellore, India.
            </li>
            <li> <i class="icofont-rounded-right"></i> August 2009 – May 2013</li>
          </ul>
        </div>
      </div>
    </div>
    <!-- End Interests -->
  </section>
  <!-- End About Section -->

  <!-- Start Experience Section -->

  <section id="experience" class="services">
    <div class="container">
      <div class="section-title">
        <br>
        <h2>Experience</h2>
      </div>
      <div class="row">
        <div class="col-lg-12" data-aos="fade-up">

          <div class="col-md-12 mt-4 mt-md-0 icon-box" data-aos="fade-up" data-aos-delay="100">
            <h5 style="text-align: left">Medicare (Baltimore, MD) | Apr 2021 – Present </h5>
            <p style="text-align: left; color: #172615; font-size: medium">
              <em> Data Engineer</em>
            </p>
            <ul style="text-align: left">
              <li>
                Worked on complex SQL Queries, PL/SQL procedures and convert them to ETL tasks.
              </li>
              <li>
                Built data pipelines to move data from source to destination scheduling by Airflow.
              </li>
              <li>
                Developed BIX Extract application in Python to ingest Pega (Complaint System) files to HDFS and
                configure Airflow DAGs to orchestrate ETL workflow.
              </li>
              <li>
                Involved in Agile Development process (Scrum and Sprint planning).
              </li>
              <li>
                Involved in various sectors of business, with In-depth knowledge of SDLC (System Development
                Life Cycle) with all phases of Agile - Scrum, & Waterfall.
              </li>
              <li>
                Developed Map Reduce jobs using Java to process large data sets by fitting the problem into the
                Map Reduce programming paradigm.
              </li>
              <li>
                Developed Spark scripts by using Java, and Python shell commands as per the requirement.
              </li>
              <li>
                Worked with CI/CD tools such as Jenkins and version control tools Git, Bitbucket.
              </li>
              <li>
                Worked on source control tools like Tortoise SVN, CVS, IBM Clear Case, Perforce, and GIT.
              </li>
              <li>
                Created pipelines, data flows and complex data transformations and manipulations using ADF and
                PySpark with Databricks
              </li>
              <li>
                Used the RUP and agile methodology to conduct new development and maintaining software.
              </li>
              <li><b>Environment : </b>SOAP, REST APIs, SQL, Azure, ETL, APIs, cloud, UNIX, PL/SQL, CI/CD, Matplotlib,
                PyHive, Keras, Java, NoSQL- HBASE, Sqoop, Pig, MapReduce, Oozie, Spark MLlib</li>
            </ul>




          </div>

          <div class="col-md-12 mt-4 mt-md-0 icon-box" data-aos="fade-up" data-aos-delay="100">

            <h5 style="text-align: left">Fidelity Investments (Durham, NC) | Dec 2019 to Mar 2021
            </h5>
            <p style="text-align: left; color: #172615; font-size: medium">
              <em> Data Engineer</em>
            </p>
            <ul style="text-align: left">
              <li>
                Designed & developed batch processing solutions by using Data Factory and Azure Databricks.
              </li>
              <li>
                Designed, developed and implemented solutions with data warehouse, ETL, data analysis and BI
                reporting technologies.
              </li>
              <li>
                Identified, evaluated, and documented potential data sources in support of project requirements
                within the assigned departments as per agile methodology.
              </li>
              <li>
                Created Python / SQL scripts, to transform Databricks notebooks from Redshift table into Snowflake
                S3 buckets.
              </li>
              <li>
                Extensively worked on Data Services for migrating data from one database to another database.
              </li>
              <li>
                Implemented various performance optimization techniques such as caching, Push-down memoryintensive
                operations to the database server, etc.
              </li>
              <li>
                Worked with developing customized UDF's in java to extend Hive and Pig Latin functionality
              </li>
              <li>
                Involved in data from RDBMS and performed data transformations, and then export the transformed
                data to Cassandra as per the business requirement and used Cassandra through Java services
              </li>
              <li>
                Involved in Agile development methodology active member in scrum meetings
              </li>
              <li>
                Involved in continuous integration and deployment (CI/CD) using DevOps tools like Looper,
                Concord. Designed a workflow using Airflow to automate the jobs.

              </li>
              <li>
                Implemented a CI/CD pipeline with Jenkins, GitHub, Nexus, Maven and AWS AMI'S.

              </li>
              <li>
                Created several Databricks Spark jobs with Pyspark to perform several tables to table operations
              </li>
              <li>
                Designed and Implement test environment on AWS.
              </li>


              <li><b>Environment : </b>Python, AWS S3, AWS Redshift, AWS Data Pipeline, Spark, CI/CD, IBM DB2, Airflow,
                SAP
                ECC, SQL, Agile, ELT, S3, SOL DB, SQL AZURE, AWS.</li>
            </ul>




          </div>

          <div class="col-md-12 mt-4 mt-md-0 icon-box" data-aos="fade-up" data-aos-delay="100">
           
            <h5 style="text-align: left">AbbVie (Chicago, IL) | May 2018 – Nov 2019</h5>
            <p style="text-align: left; color: #172615; font-size: medium">
              <em> Data Engineer</em>
            </p>
            <ul style="text-align: left">
              <li>
                Participated in requirements sessions to gather requirements along with business analysts and
                product owners. Involved in Kafka and building use case relevant to our environment.
              </li>
              <li>
                Worked on implementation and maintenance of Cloudera Hadoop cluster.
              </li>
              <li>
                Pulled the data from data lake (HDFS) and massaging the data with various RDD transformations
              </li>
              <li>
                Involved in building an information pipeline and performed analysis utilizing AWS stack (EMR, EC2,
                S3, RDS, Lambda, Glue, SQS, and Redshift).

              </li>
              <li>
                Responsible for developing data pipeline using flume, Sqoop and pig to extract the data from
                weblogs and store in HDFS
              </li>
              <li>
                Developed Oozie workflow jobs to execute hive, Sqoop and MapReduce actions
              </li>
              <li>
                Architected, Designed and Developed Business applications and Data marts for reporting.
              </li>
              <li>
                Imported the data from different sources like HDFS/HBase into Spark RDD and developed a data
                pipeline using Kafka and Storm to store data into HDFS
              </li>
              <li>
                Collaborated with Business users for requirement gathering for building Tableau reports per
                business needs.
              </li>
              <li>
                Developed Pig Latin scripts for replacing the existing legacy process to the Hadoop and the data is
                fed to AWS S3.
              </li>
              <li>
                Developed continuous flow of data into HDFS from social feeds using Apache Storm Spouts and
                Bolts. Involved in loading data from UNIX file system to HDFS.
              </li>
              <li>
                Implemented the Big Data solution using Hadoop, hive and Informatica to pull/load the data into the
                HDFS system.
              </li>
              <li>
                Objective of this project is to build a data lake as a cloud-based solution in AWS using Apache
                Spark. Installed and configured Hadoop Ecosystem components
              </li>
              <li>
                Developed Spark code using Scala for faster testing and processing of data.

              </li>
              <li>
                Apache Hadoop installation & configuration of multiple nodes on AWS EC2 system
              </li>
              <li>
                Created Hive External tables to stage data and then move the data from Staging to main tables.
              </li>
              <li>
                Documented the requirements including the available code which should be implemented using
                Spark, Hive, HDFS, HBase and Elastic Search.
              </li>

              <li><b>Environment : </b>Hadoop, YARN, HDFS, Spark, flume, AWS, Sqoop, pig, MapReduce, UNIX, Zookeeper
                HBase, Kafka, Scala, NoSQL, Cassandra, Elastic Search, Sqoop</li>
            </ul>
          </div>

          <div class="col-md-12 mt-4 mt-md-0 icon-box" data-aos="fade-up" data-aos-delay="100">

            <h5 style="text-align: left">Progressive Corporation (Mayfield, OH) | Mar 2016 – Apr 2018</h5>
            <p style="text-align: left; color: #172615; font-size: medium">
              <em> Data Engineer</em>
            </p>
            <ul style="text-align: left">
              <li>
                Worked closely with Business Analysts to gather requirements and design a reliable and scalable
                data pipelines.

              </li>
              <li>
                Develop and add features to existing data analytic applications built with Spark and Hadoop on a
                Scala, Python development platform on the top of AWS services
              </li>
              <li>
                Programming using Python, Scala along with Hadoop framework utilizing Cloudera Hadoop
                Ecosystem projects (HDFS, Spark, Sqoop, Hive, HBase, Oozie, Impala, Zookeeper etc.).

              </li>
              <li>
                Involved in developing spark applications using Scala, Python for Data transformations, cleansing
                as well as validation using Spark API.
              </li>
              <li>
                Developed several Accelerators/Tools as Spark/Step Functions for Workflow (UNIX Scripts as well)
                applications that saved a lot of Manual Efforts
              </li>
              <li>
                Developed ETL data pipelines using PySpark on AWS EMR, also Configured EMR clusters on
                AWS.
              </li>
              <li>
                Provided end to end data solutions to business and analytics team ensuring end to end encryption
                by leveraging AWS cloud services and native python and shell scripting.
              </li>
              <li>
                Involved in trouble shooting spark jobs with the help of Spark UI and monitored spark jobs
              </li>
              <li>
                Setup continuous integration/deployment of spark jobs to EMR clusters (used AWS SDK CLI)
              </li>
              <li>
                Integration of data storage solutions in spark – especially with AWS S3 object storage.

              </li>
              <li>
                Worked on all the Spark APIs, like RDD, Data frame, Data source and Dataset, to transform the
                data
              </li>
              <li><b>Environment : </b> Hadoop 2.7.7, HDFS 2.7.7, Apache Hive 2.3, Apache Kafka 0.8.2.X, Apache Spark 2.3,
                Spark-SQL, Spark-Streaming, Zookeeper, Pig, Oozie, Java 8,11, Python3, S3, EMR, EC2, Redshift,
                Cassandra, Nifi, Talend, HBase, Cloudera (CHD 5.X)</li>
            </ul>
          </div>

          <div class="col-md-12 mt-4 mt-md-0 icon-box" data-aos="fade-up" data-aos-delay="100">

            <h5 style="text-align: left">DataFactZ (Hyderabad, India) | May 2014 – Aug 2015</h5>
            <p style="text-align: left; color: #172615; font-size: medium">
              <em> Data Engineer</em>
            </p>
            <ul style="text-align: left">
              <li>
                Worked extensively along with business analysis team, scrum masters in gathering requirements
                and understanding the workflows of the organization

              </li>
              <li>
                Involved in Data mapping specifications to create and execute detailed system test plans. The data
                mapping specifies what data will be extracted from an internal data warehouse, transformed and
                sent to an external entity.

              </li>
              <li>
                Analyzed business requirements, system requirements, data mapping requirement specifications,
                and responsible for documenting functional requirements and supplementary requirements in
                Quality Center.
              </li>
              <li>
                Wrote and executed unit, system, integration and UAT scripts in a data warehouse projects
              </li>
              <li>
                Wrote and executed SQL queries to verify that data has been moved from transactional system to
                DSS, Data warehouse, data mart reporting system in accordance with requirements.
              </li>
              <li>
                Created the test environment for Staging area, loading the Staging area with data from multiple
                sources.
              </li>
              <li>
                Worked on data profiling and data validation to ensure the accuracy of the data between the
                warehouse and source systems.

              </li>
              <li>
                Monitored the Data quality of the daily processes and ensure integrity of data was maintained to
                ensure effective functioning of the departments.
              </li>
              <li>
                Developed data mapping documents for integration into a central model and depicting data flow
                across systems & maintain all files into electronic filing system.

              </li>
              <li><b>Environment : </b> Oracle 9i, SQL, DB2, XML, ad hoc, Excel 2008, Data Validation</li>
            </ul>
          </div>

        </div>
      </div>
    </div>
  </section>

  <!-- End Experience Section -->



  <!-- Start Skills Section -->

  <section id="skills" class="services">
    <div class="container">
      <div class="section-title">
        <br>
        <h2>Skills</h2>
      </div>
      <div class="row">
        <div class="col-lg-12" data-aos="fade-up">
          <ul style="list-style:  none; padding-left: 20px; ">

            <li><i class="icofont-rounded-right"> </i><b>Languages:</b> Shell scripting, SQL, PL/SQL, Python, R,
              PySpark, Pig, Hive
              QL, Scala, Regular Expressions</li>
            <br>
            <li> <i class="icofont-rounded-right"> </i><b>Hadoop Distribution:</b>Cloudera CDH, Horton Works HDP,
              Apache, AWS

            </li>
            <br>
            <li><i class="icofont-rounded-right"> </i><b>Big Data Ecosystem:</b>HDFS, MapReduce, Hive, Pig, Sqoop,
              Flume, Oozie,
              Zookeeper, Kafka, Cassandra, Apache Spark, Spark
              Streaming, HBase, Flume, Impala
            </li>
            <br>
            <li><i class="icofont-rounded-right"></i><b>Databases :</b> Oracle 10g/11g/12c, SQL Server, MySQL,
              Cassandra,
              Teradata, PostgreSQL, MS Access, Snowflake, NoSQL
              Database (HBase, MongoDB)
            </li>
            <br>
            <li><i class="icofont-rounded-right"></i><b>Cloud Technologies:</b>Amazon Web Services (AWS), Microsoft
              Azure, GCP</li>
            <br>
            <li><i class="icofont-rounded-right"></i><b>Version Control :</b> GIT, GIT HUB

            </li>
            <br>
            <li><i class="icofont-rounded-right"></i><b>IDE & Tools, Design:</b>Eclipse, Visual Studio, Net Beans,
              Junit, CI/CD, SQL
              Developer, MySQL, SQL Developer, Workbench, Tableau</li>
            <br>
            <li><i class="icofont-rounded-right"></i><b>Operating Systems:</b>Windows 98, 2000, XP, Windows 7,10, Mac
              OS, Unix, Linux</li>
            <br>
            <li><i class="icofont-rounded-right"></i><b>Data Engineer/Big Data
                Tools/Cloud/ETL/Visualization/Other
                Tools
                :</b>Databricks, Hadoop Distributed File System (HDFS), Hive,
              Pig, Sqoop, MapReduce, Spring Boot, Flume, YARN,
              Hortonworks, Cloudera, MLlib, Oozie, Zookeeper, etc. AWS,
              Azure Databricks, Azure Data Explorer, Azure HDInsight,
              Linux, Bash Shell, Unix, etc., Tableau, Power BI, SAS,
              Crystal Reports, Dashboard Design
            </li>



          </ul>
        </div>
      </div>
    </div>
  </section>

  <!-- End Skills Section -->

  <!-- Start Links section -->
  <section id="links" class="services">
    <div class="container">
      <div class="section-title">
        <h2>Resume & Links</h2>
      </div>
      <div class="row">
        <div class="col-md-3 mt-4 mt-md-0 icon-box" data-aos="fade-up" data-aos-delay="100">
          <a href="./assets/resume/Venkat_Dutt.pdf" target="_blank">
            <div class="icon"><i class="icofont-page"></i></div>
          </a>
          <h4 class="title">
            <a href="./assets/resume/Venkat_Dutt.pdf" target="_blank">Resume</a>
          </h4>
          <p class="description" style="color: #fff">
            The link contains downloadable resume
          </p>
        </div>
      
      </div>
    </div>
  </section>
  <!--End Links Section -->

 

  <!-- Vendor JS Files -->
  <script src="assets/vendor/jquery/jquery.min.js"></script>
  <script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="assets/vendor/jquery.easing/jquery.easing.min.js"></script>
  <script src="assets/vendor/php-email-form/validate.js"></script>
  <script src="assets/vendor/waypoints/jquery.waypoints.min.js"></script>
  <script src="assets/vendor/counterup/counterup.min.js"></script>
  <script src="assets/vendor/owl.carousel/owl.carousel.min.js"></script>
  <script src="assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
  <script src="assets/vendor/venobox/venobox.min.js"></script>
  <script src="assets/vendor/typed.js/typed.min.js"></script>
 

  <!-- Template Main JS File -->
  <script src="assets/js/main.js"></script>
</body>

</html>